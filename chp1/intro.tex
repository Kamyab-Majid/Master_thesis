\chapter{Introduction}

\section{Autonomous UAV}

Unmanned aerial vehicles (UAVs) are aircraft with no human on board. They are controlled remotely or automatically. Unmanned Aerial Vehicles (UAV) are gaining popularity, both in terms of academic research and potential applications \cite{valavanis2015handbook}. 

Classification of the UAVs has two major sub-classes of fixed-wing and rotary-wing. the rotary-wing UAVs received growing attention in recent years thanks to the improvements in embedded microprocessors and batteries.  surveillance \cite{semsch2009autonomous,puri2005survey}, disaster management \cite{maza2011experimental, birk2011safety}, and
rescue missions \cite{alotaibi2019lsar} are only a few numbers of examples of the broad implementation field of the rotary-wing UAVs.  

The majority of recent years' research is focused on quadcopters which are rotary-wing aircraft with four rotors \cite{luukkonen2011modelling, gheorghictua2015quadcopter, wang2016dynamics, bashi2017unmanned} Thanks to their agility and ease of control. On the other hand, single rotor helicopters have gotten less attention from researchers, mainly because they are intrinsically unstable; they have highly coupled nonlinear dynamics, and wind gusts can easily disturb them.  

The helicopter is the principal representation of the rotary wing family. The conventional helicopter layout has two engine-powered rotors: the main rotor and the tail rotor. The main rotor generates the thrust power for the helicopter's elevation. The tail rotor offsets the main rotor torque and maintains the helicopter orientation. The change in body orientations of the helicopter results in the inclination of the main rotor, and therefore generating the propulsive force for the helicopter's longitudinal/lateral movement. 


All flying features and physical principles of their full-sized counterpart are retained by small helicopters. Moreover, in comparison to full scale helicopters, they are inherently more manoeuvrable and competent. Due to their satisfactory flying ability, size and low expense, UAV science community has engaged in developing minimal cost and reliable autonomous navigation technologies. 

Four control inputs are used for the helicopter. Two cyclic controls which handle the helicopter's longitudinal/lateral movement, a collective control of vertical movement and, lastly, the control of pedal control of the helicopter's heading movement. unrestrained helicopter movement is governed by an underactuated structure, in which the number of control inputs (4) is less than the number of degrees of freedom to be controlled (6 DOF), making it difficult to use the traditional approach for controlling Euler–Lagrange systems (which is usually used in the industrial automation).  For these reasons, much research has concentrated on control method for unmanned drones that ensures stability and durability. These factors lead to a complex control problem for single rotor small-scaled helicopters. However, the payload capacity of these helicopters is superior to quadcopters, making them more suitable for transportation in emergency situations \cite{quan2017introduction}. As single rotor small, scaled helicopters received less attention, in this study, we will focus on this type of UAVs. 

The exact dynamics of the helicopter are unidentified and represented using mechanical relevant mathematical formulas of lesser order, as in most engineering disciplines. It should be emphasized that the estimated model is simply a "abstract concept" since a comprehensive description of the real dynamics of the helicopter is almost infeasible \cite{ren2012modeling}. 

As a single-rotor helicopter is unstable by nature, it requires a flight control system that operates the vehicle, which is like a human pilot in a large, scaled helicopter. As a result, the flight control can either accept remote control input from an operator or operate autonomously. Remote control of single rotor helicopters is not economically viable, so autonomous control is preferred for most commercial applications. Therefore, the autonomous control of unmanned aerial vehicles (UAVs) is the goal of this research.  

\section{Traditional Control Systems}

Control of single rotor helicopters is studied through classic (continuous) or modern (digital) control approaches. Most helicopter systems are inherently non-linear, with non-linear differential equations specified for their dynamics. Researchers, however, generally construct linearized helicopter systems models for analytical purposes. In particular, if this system runs around an operational point and the signals involved are minor, a linear model that estimates a certain non-linear helicopter system may be produced. A large number of approaches have been suggested by researchers for the design and study of control systems for linear systems. 

Traditional flight control systems are primarily classified as linear or nonlinear. This categorization is often based on the rotor-craft model expression provided by the controller. Linearization designs are more application-focused and have been used on the majority of helicopter models. Their appeal derives from the ease of control, which reduces both computation cost and duration of the project. 

In general, most control systems are based on the broadly established idea of stabilization derivatives, utilizing a linear system of helicopter dynamics. However, a substantial study has been carried out in recent years on non-linear dynamic formulations in the context of helicopter control flight. The concepts of nonlinear controllers are mostly assessed for their conceptual framework to the problem of helicopter navigation. Their application remains a major issue, mostly because of the control system's increasing order and complex nature. Its contribution, however, is crucially important to understand the constraints and possibilities of helicopter navigation.

A linear Multiple-Input Multiple-Output (MIMO) coupled helicopter model serves as the foundation for the linear controller architecture. The internal model method and integral control design are two common design strategies for dealing with the trajectory tracking of linear systems. The proposed control method has the drawback of being complicated to build, whereas integral control is limited to instances where the reference output is a continuous signal. The key principle underlying the linear controller design is to identify the desired state vector for each of these two subsystems, such that when the helicopter status variables converge with their intended state values, the tracking error asymptotically converges to zero. For each subsystem, the desired state vectors are components and higher derivatives of the reference output vectors. 

The linear H$\infty$ control theory is used for a linear helicopter model such as the one done. However, control laws based on linear helicopter dynamics is not globalized since it shows desired behavior just around a region of operation. This has led to a large number of studies using non-linear control approaches to implement dynamic helicopter models. The feedback linearity control for trajectory tracking was implemented based on a lower order component of the Lagrangian helicopter model \cite{vilchis2003nonlinear}. 

Because of its highly cross-coupling nature of single rotor small scale helicopters (SRSSH), usually, a MIMO approach is implemented \cite{koo1998output, mahony1999hover}. H$\infty$ method is also used in \cite{la2003integrated,civita2006design} using a 30-state nonlinear model by an inner loop and outer loop technique. Sliding mode controller is also used for control of SRSSH \cite{khaligh2014control}. 

Controller design approaches ignore the multivariate character of rotor-craft dynamics as well as the strong link between rotorcraft variables and control inputs. In this sort of framework, each control input is in charge of regulating a single rotorcraft outlet.  interconnections between rotorcraft outputs are ignored, and each control input is linked to a SISO feedback loop. The SISO feedback mechanisms associated with the control inputs are totally independent of one another. The SISO feedback mechanisms are built using standard looping platforms \cite{walker1996advanced}. The amplitude and gain tolerances of a feedback loop determine the other's stability. These tolerances define the amount of amplitude and timing that the controller may inject to keep the feedback cycle dynamics constant. However, in the case of multivariable systems, these tolerances can readily lead to erroneous findings. 

An 11 state linear model was developed to examine the feedback controller features of the PID technique \cite{mettler1999system}. Based on the prediction error technique, a time-domain identification procedure was used to identify the set of parameters. The PID design proved unable to reduce the mutual coupling among helicopter's lateral and longitudinal movements, and the aircraft control system was confined to standstill flying. The obtained findings revealed that SISO strategies have mediocre reliability and that multidimensional procedures are essential to minimize the helicopter dynamics' intrinsic strongly coupled impact. 

Because of the lag time between the helicopter's translational and attitude subsystems, most linear control schemes employ a multi-loop control method \cite{kim2003flight, johnson2005adaptive, marconi2007robust}. Each input controls one helicopter output via a single-input single-output (SISO) feedback system, and the helicopter's attitude equations are separated from translational motion using two primary control loops. The slower outer-loop regulates the helicopter's heaving, longitudinal, and lateral movements by computing the needed collective input and attitude angles to guide the aircraft along its intended route. The basis inputs to the inner feedback loop are then these desirable attitude angles. The inner-loop is used to regulate the helicopter's attitude, which moves at a considerably quicker rate than the translational motion. 

A linearized model of the helicopter dynamics is used in the multi-loop approach and the cross-couplings between different DOFs are neglected. Since the cross-coupling dynamics are important, this often results in poor performance of the controller. To account for the cross-couplings that exist between different DOFs of the helicopter, a multi-input multi-output (MIMO) control approach has been used in recent years \cite{koo1998output,raptis2009system}. 

 Koo et al. use the input-output feedback linearization technique to provide a MIMO solution for the control of small-scale helicopters. The helicopter dynamics are not linearized by the accurate input-output linear system, resulting in instability zero dynamics. The zero dynamics are then stabilized in the simulated world by ignoring the connections between moments and forces and utilizing approximate input-output linearization to obtain limited tracking. Instead of controllable inputs like the collective, cyclic, and pedal inputs, unrealistic control inputs like the gradients of the main and tail rotor thrust and the flapping angles are employed to describe the system \cite{koo1998output}. 

The influence of thrust force components associated with the primary rotor disc displacement is ignored by most nonlinear dynamic systems. These parasite forces have a minimum impact on movement dynamics. This is standard procedure. This approximation leads to several mathematical models with a response form appropriate for backstep control designs laid forth in \cite{krstic1995nonlinear} and numerous researchers used this procedure \cite{fantoni2002non,azzam2010quad,mahony2004robust}. 

Mahony et al. described a MIMO strategy for controlling small-scale aircraft in hover using a backstepping mechanism \cite{mahony1999hover}. To do this, the flapping behaviors and friction forces are ignored, and the control design is based on a mathematical model of the helicopter dynamics around hover. In a study done by Raptis et al., a time-dependent backstepping approach is used to create a MIMO control scheme for a small-scale helicopter \cite{raptis2009system}. Simplifying hypotheses are used to generate the helicopter's dynamic model in a cascading design appropriate for the backstepping control scheme. For instance, in all aviation phases, induced velocity is considered to be constant and the impacts on the thrust computations of the vehicle velocity are disregarded so that main and tail rotors are respectively proportionate in proportion to the input of collectives and pedals. The main and tail rotors' drag torque is also disregarded.

Another non-linear control scheme is given in a work by Godbolt et al. \cite{godbolt2013experimental} employing a cascade method. In order to unite attitude and movement dynamics, the internal loop control mechanism is utilized. The control design uses simplification principles. For example, due to the rigidity of the main rotor shaft, the contributions of the rolling and pitching moments to the fuselage dynamic attitude are ignored. Also, because of the rotor blowing in the translational dynamics, it neglects the influence of smaller body forces. A nonlinear control technique is then taken into account to offset the tail rotor's impacts to small friction forces. 

An H$\infty$ controller's usual construction consists of two components. The first element consists of Proportional Integral compensators and low pass filters in a manner similar to the traditional approaches of single input single output systems. The Proportional Integral compensators enhance the system's low-frequency gain, reducing disturbances, and attenuating steady-state error. The low pass filters are generally employed for noise reduction. The second element of the control is the H1 synthesis component, which is determined by a constant signal gain for stabilizing multi-functional dynamic response, as well as being appropriate for a performance criterion \cite{kim2003flight,khalil1996robust}.

A single value loop forming process based on two degrees of H$\infty$ freedom was created in the research done by Walker et al. \cite{walker1996advanced} which is an observation basis multivariate controller. The controls were to build a complete autopilot system for a helicopter. The flying system is incorporated with piloted aviation operations, as opposed to automated flight technologies. The aim of the remote control is for the helicopter to monitor the pilot's control input and speed control. The control scheme is designed to eliminate the connection between axes of helicopter dynamics, therefore lowering the burden of the pilot. The pilot is alone responsible for generating the benchmark and high-speed controls that are required to move the aircraft.

An innovative architecture of static H$\infty$ output controls was given to stabilize an autonomous helicopter in a hovering  cite{gadewadikar2009h}. The optimum control technology makes it possible to devise multivariate feedback systems that enhance the rank of the control unit utilizing fewer states. The structure of the controller feedback loops coincided with the actual flight experience of the helicopter such that the controller's design was acceptable. The H$\infty$ control system form decreases the influence on high-frequency Helicopters of un-modeled dynamics.

In a research by Kendoul et al.  \cite{kendoul2007real} the control design for a Yamaha R-50 helicopter using H$\infty$ loop forming technology is provided. The control design is composed of non 30-state model of helicopter dynamics in an internal loop approach that is linearized by various operational positions in the desired trajectory envelope. Then an H$\infty$ loop-fitting controller is built to cover this required flying area based on the acquired linear models.  

The UAV control scheme is studied in \cite{kim2002nonlinear} for a non-linear trajectory tracking control. The non-linear model of helicopter dynamics is discretized and the tracking control issue is then formulated to reduce costs using a quickly converged steepest descent approach. The primary problems of application are the coordination of the cost weight matrix and the constants in the probability density. 

In the majority of situations, three nonlinear matrix expressions are required to solve the final loop control issue. In \cite{gadewadikar2008structured}, the H$\infty$ synthesis portion of the controller was resolved by solving just two paired matrix formulas that do not need the information of the initial stabilization gain. There are two principal loops in the control system framework. The first loop is capable of stabilizing the dynamic behavior of the arrangement, and the second loop is for position monitoring. A 13-state linear model of the coupling fuselage and rotor dynamic is the architecture of the control unit. The sequence and structure of the model were adopted in \cite{mettler2013identification}.

In another study, Riccati Equation concept is provided \cite{bogdanov2007state}. The complicated dynamics of the helicopter are modified to a pseudo linear, state-dependent (SDC) coefficient and a feedback-optimum matrix is produced at all times by solving the LQR equation. Because there are many non-parametric terms in SDC form and the fact that the helicopter model is not aligned in terms of the control system, it is ignored to achieve a control-affine SDC helicopter dynamics framework necessary for SDRE control designs in certain non-linearity models. A non-linear compensation is then built to increase the control signal to roughly cancel ignored nonlinear effects. 

Owing to its resilience with boundary parameter uncertainties, the sliding mode controller can be another non-linear, small-scale, unmanned helicopter management MIMO method. A robust, nonlinear, sliding mode controller flight control is given in \cite{ifassiouen2007robust} for a compact, standalone hover helicopter. The dynamics of the nonlinear helicopter are initially oversimplified by disregarding the drag torque of the rotors and the rear and the connections of the aerodynamic forces and momentum. Then the linearized model is transformed for a squared model into a linear system. For input refined systems, untrue control inputs such as the rolling, pitch, and yaw moments are considered instead of the actual control inputs, and the gradient is considered to be the primary rotor thrust.

The Translational Rate Control (a technique for a UAV is detailed in a study by Pieper et al. \cite{pieper1995application}) for another sliding mode controller approach in hovers. A fundamental, linearized model of the hovering helicopter and a Sliding mode controller is built to comply with the operating quality requirements for the Translational Rate Control hover control system. 

A reference model sliding mode controller design is detailed in a study by Wang et al. \cite{wang2008model} and a multi-loop control method is employed to regulate the hover of a UAV. The non-linear helicopter model is modeled linearly around the hover and the coupling movement of the helicopter is ignored, to treat every DOF as a self-contained SISO system. The PID technique is then developed for each of the longitudinal and the lateral controller designs and heavily loaded loops. 

Another sliding mode controller technique is presented in a research for controlling a UAV \cite{fu2012chattering}. In this method, the DOFs of helicopter movement are decoupled in these three principal feedback loops: position, speed, and orientation loop. To get an appropriate form for the sliding mode controller method, the Equations of each loop are simplified. For instance, for the Euler angles in the speed cycle, the small-angle presumption is utilized to linearize the equations and get an input-affined shape. A sliding mode controller for each loop is then designed. 

A small-scale autonomous helicopter group control is presented adopting a sliding mode controller approach \cite{fahimi2008full}. In order to produce arbitrary tri-dimensional formations, a sliding mode controller is established for each technique, and the training will be maintained by two leaders/follower controllers. The rotor's flapping complexities are ignored and unrealistic control inputs including the main and tail rotor thrust and pitch and roller moments are applied to describe the system in an input-affine manner instead of actual controlled inputs. The square shape is then exploited to get the control design using a reference points technique. 

The aerodynamics of the helicopter is separated into three components with slow, medium, and rapid modes with a multiple time control based on the technique of the slider mode controller \cite{xu2010multi}. In all flying regimes, nonlinearities of the main and tail rotor intake are removed and the induced speed is presumed to be fixed. A nonlinear controller is built with a sliding mode controller for each mode and results for simulation are provided. Nevertheless, for controls that may result in a non-unique solution, the slow mode controller requires an iterative process. 

It is vital that the control architecture is strong enough in the case of the helicopter which has considerable uncertainty. In the presence of parametric and model uncertainty, there is a design that ensures limited traceability \cite{isidori2003robust}. The suggested control scheme includes stabilizing strategies for input saturation feedback systems as well as adaptive nonlinear output control techniques.

In another study, the helicopter model includes the dynamic behavior of the helicopter movement equations that are augmented by a modified aerodynamic force and torque generating model. The Helicopter Dynamics nonlinear model is presented in \cite{koo1999differential}. In most studies into the design of a non-linear helicopter controller, this particular model was used. The precise linearization input-output fails to linearise the model of the helicopter which leads to instability of zero dynamics. The usage of the approximation model, which does not consider the thrust forces created by the main rotor flap movement, has also been demonstrated to be fully linearized.


In \cite{koo1998output}, an approximation linearization in input-output was used to achieve a helicopter system that is dynamically linear without zero dynamics and that has the required characteristic of relative smoothness. The difficulty of an oscillatory ship deck helicopter landing \cite{isidori2012robust} has been appropriately controlled using a conceptual representation. In \cite{kadmiry2004fuzzy}, the design of a floating flight controller for the unmanned APID-MK3 helicopter is described with a unique approach. 


In the literature, the majority of control schemes, including Multi-loop and MIMO, are implementing the linear model of the helicopter under various trimming requirements, instead of using the non-linear model directly. This confines the correctness of the linear model to the neighborhood of its linearization of the trimmed requirements. Several linear models are therefore necessary to cover a variety of flying regimes and several gain programmed controllers are required in all such regimes to control the helicopter \cite{downs2007control}. 

Aerodynamic forces and moments fluctuate substantially across different flying circumstances due to the complicated aerodynamic performance of helicopter thrust output. These approximations are not desired for managing an autonomous helicopter over a broad variety of flying phases, through linear system and/or rejection of non-linear components \cite{raptis2011linear}. 

The issue of optimal control methods is that they all necessitate knowledge of the robot's dynamics, requiring system identification and model derivation for each UAV. Depending on the task, this can become tedious, if not impossible. Notably, the final control system will be a one-of-a-kind solution to a specialized study. These strategies may be insufficient to deal with changing conditions, unanticipated events, and stochastic environments \cite{zhou2019vision}. 

Previous approaches to nonlinear control using neural networks and nonlinear inversion were published in \cite{johnson2005adaptive}. Nonlinear control approaches have also been presented. In all situations, the requirements for nonlinear inversion and the increase of a NN raise the controller's order substantially. In this way, it becomes impossible to derive the controller from the helicopter's non-linear governing equations. Consequently, these cases have used developed controls based on the helicopter's linearization dynamics. In the research of Hovakimyan et al. \cite{hovakimyan2001adaptive} the reduced model uses just the heavy and longitudinal mobility of the helicopter, which further restricts it.

In order to obtain adequate efficiency, the control strategies presented in the research stated above require accurate knowledge of the dynamic models involved. The issue is how to manage unforeseen disturbances to the nominal model in helicopter operations. Unexpected disturbances of this nature usually involve parameters and analytical uncertainty, unmodelled dynamics, and environmental disturbance. The existence of uncertainties and external disturbances can disrupt the feedback controller's operation and lead to significant deterioration. Approximation approaches utilizing artificial neural networks (NN) were suggested to address the presence of model uncertainty. In \cite{kim2004adaptive}, approximated NN-enhancing dynamic reversal was presented, while in \cite{enns2003helicopter} neuronal dynamic programming was demonstrated to be beneficial in the monitoring and trim control of the helicopter. 

On this basis, the following question is posed: What if the vehicle teaches itself how to perform a task optimally without using a model? This leads to the next section on reinforcement learning.

\section{The Use of Reinforcement Learning as an Optimal Control Method} \label{intro_rl}

Artificial intelligence (AI) has lately caused a breakthrough in various industries worldwide, ranging from engineering to medical services. Recent advancements in computer technology and data storage, along with AI's learning capacities, have propelled AI to the forefront of numerous applications, such as object recognition and natural language processing. AI is expected to contribute more than 15 trillion USD to the global economy while increasing GDP by 26\% by 2030. Overall, artificial intelligence (AI) is a powerful tool that covers many aspects of nowadays scientific achievements \cite{anand2019s}. 

Machine learning (ML) is arguably the most significant branch of AI. It is described as an ability in computer systems that allows them to learn without the need for continuous control over it \cite{pandey2021machine}. The area of machine learning may be divided further into supervised learning, unsupervised learning, semi-supervised learning, and reinforcement learning. 

The term "supervised learning" refers to a situation in which the "experience," or training example, provides essential information that is absent in the unknown "test examples" whereby the learned knowledge is to be implemented. An expert provides the additional information in experience. It tries to generalize across experiences and then applies this knowledge to predict labels for test examples \cite{shalev2014understanding}. Since the agent tries to mimic the expert, it will not wholly provide the same response as the expert. This error is called the Bayes error rate \cite{ng2017machine}.  

In unsupervised learning, there is no distinction between training data and test data. A typical example of such a job is grouping data collection into subgroups of related objects. Semi-supervised learning is a combination of supervised learning and unsupervised learning. During training, semi-supervised learning mixes a small quantity of labeled data with a lot of unlabeled data, which will improve learning accuracy. 

Ideally, supervised learning or semi-supervised learning can completely replicate the supervisor. However, it cannot outperform the supervisor in terms of outcomes. Reinforcement learning (RL) attempts to solve this dilemma by substantial changes to the learning process. Ultimately, the objective of RL is to enable machines to outperform all existing approaches. The RL agent tries to achieve a better result than the currently feasible ones by learning the best mapping of states to actions using a reward signal as a criterion. RL methods allow a vehicle to discover an optimal behavior on its own through trial-and-error interactions with its surroundings. This is based on the commonsense idea that if an action results in a satisfactory or better situation, the tendency to perform that action in the initial situation is \textit{reinforced}. 

RL is like classical optimal control theory \cite{sutton2018reinforcement} in engineering platform. Both theorems deal with the problem of determining an input (i.e., optimal controller in control theory or optimal policy in RL) for solving the optimization problem. Furthermore, both rely on a system's notation being described by an underlying set of states, actions, and a model that captures transitions between one state and the other. So RL can tackle the same problem that optimal control does \cite{nian2020review, powell2012ai}. However, because the agent does not have access to the state vector dynamics, the agent must learn the repercussions of its actions via trial and error while interacting with the environment. 

Although there are some recent achievements on model-based RL \cite{kaiser2019model}, most of the RL algorithms are model-free. They attempt to control without the knowledge of a dynamic model; in other words, it only receives the current states\footnote{in the fully observable Markov decision process (FOMDP). In the partially observable Markov decision process, a history of states is required in each step.} and a reward from the environment (helicopter in this case) in each step.  

This framework has received much attention in recent years, with promising outcomes in a range of domains, including outperforming human specialists on Atari games \cite{mnih2013playing}, Go \cite{silver2017mastering}, and replicating complex helicopter maneuvers. \cite{abbeel2007application, ng2006autonomous,ng2003autonomous} . A remarkable range of robotics challenges may be conveniently formulated as reinforcement learning problems dating back to 1992 when the OBELIX robot is trained to push objects \cite{mahadevan1992automatic}. A model-free policy gradient technique was used to teach a Zebra Zero robot arm how to perform a peg-in-hole insertion task \cite{gullapalli1994acquiring}. 

Recently, RL-based UAV control has received a lot of interest. The initial research generated an engineered reward function. They developed a model of robot dynamics through demonstration but then employed the model in simulation, leading to the simulation of robot state while using RL to optimize a NN controller for autonomous helicopter flying \cite{bagnell2001autonomous} or inverted helicopter maneuver \cite{ng2006autonomous}. However, defining the reward function could be an arduous task. One solution would be to utilize an expert and award the helicopter for emulating the expert's behavior. Abbeel et al. used this approach to perform aerobatic helicopter flight \cite{abbeel2007application}. 

In recent years, deep learning has been shown to improve the RL field \cite{li2017deep}. Deep learning relies on neural networks' powerful function approximation properties, which can automatically find compact low-dimensional representations of high-dimensional data (e.g., images). This enabled reinforcement learning methods to scale up to previously unreachable problems. 

Deep reinforcement learning has also gained attention recently in UAV control, William Koch et al. \cite{koch2019reinforcement} compared Deep Deterministic Policy Gradient (DDPG) \cite{lillicrap2015continuous}, Trust Region Policy Optimization (TRPO) \cite{schulman2015trust} and Proximal Policy Optimization (PPO) \cite{schulman2017proximal} algorithms on the Iris quadcopter and then comparing the result to a PID controller. Although TRPO and DDPG failed to reach stability, they have shown that PPO results are powerful enough to be comparable to a PID controller. Barros and Colombini \cite{barros2020using} also proved that the Soft Actor-Critic (SAC) \cite{haarnoja2018soft} method can perform a low-level control on a commercial quad-rotor Parrot AR Drone 2.0. However, there is still a lack of research on a small-scaled single-rotor helicopter. 

\section{Simulation Environment for RL}

In RL, the amount of try and error required to learn beneficial actions is usually high. As a result, sampling the environment is the primary challenge with reinforcement learning. One way to approach this is by having parallel similar real-world environments doing the same thing \cite{levine2018learning}. However, in the case of the UAV, failure means the loss of a UAV, and hence it is costly. This problem is exacerbated by several real-world factors that make UAVs a problematic domain for RL \cite{kober2013reinforcement}. UAVs are frequently dangerous and costly to run during the initial training such that the aircraft will fail several times until it reaches a satisfactory performance. This will need high maintenance costs in addition to the original hardware expenditures. Moreover. Robotic have continuous high-dimensional state and action spaces, and finally, it requires a fast online response. As a result, the use of a simulation environment seems necessary for the initial learning procedure of an RL algorithm. 

To compensate for the expense of real-world interactions, the UAV must first learn the behavior in simulation and then transfer it to the real vehicle. Usage of a simulator provides an affordable approach in order to create samples. In a simulation, it is possible to crash the UAV as many times as needed; In addition, no safety measures must be taken for, and there would also be no lag in the process due to maintenance or any other real-world issues. Simulations are also more reproducible; For example, wind gusts are not easy to reproduce in the real world, while in simulation, the wind gust random model can be saved and reused elsewhere. 

The issue with using a simulation environment is that none of them can completely capture real-world complexity. When a policy is trained in simulation, it usually is not optimal to use in the real world \cite{zhao2020sim}. One possible solution would be to initially train the policy in simulation and then perform tuning in the real world \cite{tran2015reinforcement, tzeng2015simultaneous}.\\

\section{Thesis Objective and outline}

In this research, we wish to expand on recent research in RL, especially Deep Reinforcement Learning (DRL) to control a SRSSH. More precisely, low-level control rules are learnt directly from the UAV simulation. Notably, the purpose of this thesis is only to train the DRL technique in a simulated setting and providing proof that the aformentioned method is capable of stabilizing the unmanned small scaled helicopter in an acceptable way, leaving future work to examine the transfer to the actual world or produce more complicated maneuvers. In the following, the outline of this thesis is included.

\subsection{Chapter 2: Reinforcement Learning Background}

A wrong choice of RL method or its hyper-parameters can be time-consuming or even impossible to reach good stability of the UAV. This is because it mainly necessitates an extensive exploration of the state-space in order to extract acceptable policies. So, in the second chapter, a review of reinforcement learning methods is discussed. By providing a mathematical framework and describing essential components, this chapter includes a formal introduction to RL. Following that, the chapter provides an overview of Value-based and policy-based methods. Finally, the chapter introduces the DRL algorithm, SAC, which will subsequently be used for UAV control.

\subsection{Chapter 3: Simulation environment}

This chapter introduces the Simulated environment used for interaction with the RL method. First, the helicopter dynamics are discussed, including the forces applied to the UAV, such as fuselage and main rotor forces. Secondly, its effect on the 6 degrees of freedom (DOF) UAV is discussed. In addition a traditional control approch, more specifically sliding mode controller is introduced to compare the result of RL policy with the optimal control theory method.  Finally, the environment setup is discussed, including the actions and rewards in the RL platform.

\subsection{Chapter 4: Result and discussion}

This chapter contains the results of applying the RL algorithm on a simulated environment, as well as a discussion and comparing the result of the obtained policy with the one generated by the sliding mode controller. In addition the effect of disturbance on the controller is discussed. 

\subsection{Chapter 5: Conclusion and future work}

The conclusion and recommendations for future work are given in the final section of this chapter.
